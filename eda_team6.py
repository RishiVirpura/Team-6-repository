# -*- coding: utf-8 -*-
"""EDA_Team6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BRAf4_fuYPszV6TxdkLTKO3PHn22lA_3

#Using gdown to access our data
"""

!gdown 1MgN5RbogMOlxJ9k7WSvrGRwmiRjrOF7M

"""# Importing required libraries and packages"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error

df = pd.read_csv('/content/analytic_data2022.csv')
df.info()
df.head()

"""#Data Cleaning"""

#Removing columns that contain numerator, denominator, confidence intervals, and other features associated with 
#specific measure IDs to base our analysis on the raw values
remove = df.columns[df.columns.str.contains(pat = 'numerator|denominator|cilow|cihigh|flag|CI high|CI low|AIAN|Asian|Hispanic|Black|White|Native|wage')]
data = df[[i for i in df.columns if i not in remove]]

#the variable mapping is created in order to store the column reference
l1 = list(data.iloc[0])
l2 = list(data.columns)
variable_mapping = {l1[i]:l2[i] for i in range (len(l1))}
data.columns=data.iloc[0]
data = data[1:]

#converting metrics from object type to numeric
numeric = list(data.columns[data.columns.str.contains(pat = '^v')])
numeric.append('county_ranked')
data[numeric] = data[numeric].apply(pd.to_numeric)

data

"""#Taking into account only county level rows into account since our analysis will be based on county level"""

#filtering out rows which belong to country or state level to make sure we have uniformity in the data as the remaining 
#data is county level 
data = data[data['county_ranked']==1]
data

"""# Checking for missing values"""

#checking for Nan values throughout the columns
pd.set_option('display.max_rows', 500)
data.isnull().sum().sort_values(ascending=False)

data.shape

#counting the number of Nan values in each column
na_counts = data.isnull().sum()

#if 33 % of the data is missing in a particular column, we drop the column
data = data[[c for c in data.columns if c not in na_counts[na_counts>=1000].index]]

data

#current columns with number of missing values
data.isnull().sum().sort_values(ascending=False)

#filtering the columns where absolute value of correlation with the target variable is greater than 0.1
#as that is almost no correlation and would not contribute to preventable hospitalization
data_corr = data.corr()
data_corr_filtered = data_corr[abs(data_corr['v005_rawvalue'])>0.1]
col_names = list(data_corr_filtered['v005_rawvalue'].to_frame().index)

data = data[[	'statecode',	'countycode',	'fipscode',	'state',	'county',	'year',	'county_ranked'  ]+ col_names]

data.isnull().sum().sort_values(ascending=False)

#looking at missing values in rows
data.isnull().sum(axis=1)[data.isnull().sum(axis=1)>5].count()

#taking 5 as threshold for filtering out rows with more than 5 missing values
data = data[data.isnull().sum(axis=1)<5]
data

#we do not want to impute the our target variable, therefore dropping rows where the target variable is null
data = data[data['v005_rawvalue'].isnull()==False]
data

#making a list of all the columns with missing values
missing_list = data.columns[data.isnull().any()]
len(missing_list)

#to check the distribution and to find the best way of imputing the columns, we want to look at the distribution
#as in normal distributions we can take random values based on it's mean and standard deviation to impute
f = plt.figure(figsize=(25, 25))
f.tight_layout()
for i, column in enumerate(missing_list, 1):
    plt.subplot(6,5,i)
    # plt.title(variable_mapping[column])
    plt.xlabel(variable_mapping[column])
    sns.histplot(data[column])

"""# After looking at distributions of columns with missing values, we decided onto taking a random value from the non-null values of column in order to impute the missing values in order to maintain the distribution, refraining from disturbing the current distribution of columns"""

#almost all the variables form a normal distribution and therefore we apply imputation based on the same
for col in missing_list:
  # compute mean and std of columns with missing values
    col_mean = data[col].mean()
    col_std = data[col].std()

    # number of NaN in column
    num_na = data[col].isna().sum()

    # generate `num_na` random samples from distribution
    rand_vals = col_mean + col_std * np.random.randn(num_na)

    # replace missing values with the randomly generated values
    data.loc[data[col].isna(), col] = rand_vals

#checking distribution after imputing
f = plt.figure(figsize=(25, 25))
f.tight_layout()
for i, column in enumerate(missing_list, 1):
    plt.subplot(6,5,i)
    # plt.title(variable_mapping[column])
    plt.xlabel(variable_mapping[column])
    sns.histplot(data[column])

#last check for null values, none found! So we are good :)
data.isnull().sum().sort_values(ascending=False)

#Renaming columns based on variable_mapping created earlier in order to look at our data with context
data.rename(columns = variable_mapping, inplace=True)

#Dropping these columns as it is not helping in analysis, we have already taken county information
data.drop(['State FIPS Code',	'County FIPS Code',	'5-digit FIPS Code', 'Release Year',	'County Ranked (Yes=1/No=0)'], axis=1, inplace=True)

"""#This is our final data!"""

data

"""#EDA

# Here, in order to understand the correlation between the columns and our target class we have built the scatterplots with each column
"""

#rounding off correlation values for further analysis
correlation = data.corr()
round(correlation,2)

plt.figure(figsize=(10, 10))
plt.scatter(data['Premature age-adjusted mortality raw value'], data['Preventable hospital stays raw value'])
plt.xlabel('Premature age-adjusted mortality raw value', fontsize = 15)
plt.ylabel('Preventable hospital stays raw value', fontsize = 15)
plt.title("Relationship between Preventable hospital stays and Premature age-adjusted mortality", fontsize = 15)
#obtain m (slope) and b(intercept) of linear regression line
m, b = np.polyfit(data['Premature age-adjusted mortality raw value'], data['Preventable hospital stays raw value'], 1)

#add linear regression line to scatterplot 
plt.plot(data['Premature age-adjusted mortality raw value'], m*data['Premature age-adjusted mortality raw value']+b)

plt.figure(figsize=(10, 10))
plt.scatter(data['Diabetes prevalence raw value'], data['Preventable hospital stays raw value'], )
plt.xlabel('Diabetes Prevalence Raw Value', fontsize = 15)
plt.ylabel('Preventable Hospital Stays Raw Value', fontsize = 15)
plt.title("Relationship Between Preventable Hospital Stays and Diabetes Prevalence", fontsize = 15)
#obtain m (slope) and b(intercept) of linear regression line
m, b = np.polyfit(data['Diabetes prevalence raw value'], data['Preventable hospital stays raw value'], 1)

#add linear regression line to scatterplot 
plt.plot(data['Diabetes prevalence raw value'], m*data['Diabetes prevalence raw value']+b)

plt.figure(figsize=(10, 10))
plt.scatter(data['Poor or fair health raw value'], data['Preventable hospital stays raw value'])
plt.xlabel('Poor or Fair Health Raw Value', fontsize = 15)
plt.ylabel('Preventable Hospital Stays Raw Value', fontsize = 15)
plt.title("Relationship between Preventable Hospital Stays and Poor or Fair Health in US Counties", fontsize = 15)
m, b = np.polyfit(data['Poor or fair health raw value'], data['Preventable hospital stays raw value'], 1)

#add linear regression line to scatterplot 
plt.plot(data['Poor or fair health raw value'], m*data['Poor or fair health raw value']+b)

#plotting correlation scatter plots of all the remaining columns
f = plt.figure(figsize=(30, 30))
l1 = ['']+list(data.columns)
len(l1)
for i, column in enumerate(data.columns, 1):
    plt.subplot(10,6,i)
    plt.title(l1[i])
    plt.scatter(x=data[column], y=data['Preventable hospital stays raw value'], )
    
f.tight_layout(pad=5.0)
plt.show()

"""#In order to further look at the feature relevance, we decided onto build a model which highlights the columns with greater cofficient with the target variable (i.e. better predicts it)"""

from sklearn.model_selection import train_test_split

data

#Separating feature class and target class
to_drop = ['State Abbreviation', 'Name']
data.drop(columns = to_drop, inplace = True)
y = data['Preventable hospital stays raw value']
data.drop(columns = ['Preventable hospital stays raw value'], inplace = True)
X = data

X

#splitting into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

X_test

#standardizing the data (bring to the same scale)
# apply scaling
scaler = StandardScaler()
# fit on training dataset only 
scaler.fit(X_train) 
# transform training dataset
X_train = scaler.transform(X_train)
# transform testing dataset 
X_test = scaler.transform(X_test)

#running lasso and e_net models with different alpha values and using GridSearchCV to tune the hyperparameters
#in order to get the best model and it configuration based on R2 and RMSE values
params = {'alpha': [20, 10, 5, 1, 0.1, 0.01]}
lasso = Lasso()
e_net = ElasticNet()


for model in [ lasso, e_net]:
    cv = GridSearchCV(model, params, cv = 5, scoring = ['neg_mean_squared_error', 'r2'], refit = 'neg_mean_squared_error')
    cv.fit(X_train, y_train)
    
    RMSE = (-cv.best_score_.mean())**(1/2)
    print('model: ', model) 
    print('cross-validated mean RMSE: {:.3f}'.format(RMSE))
    mean_R2 = cv.cv_results_['mean_test_r2'].mean()
    print('cross-validated mean R2: {:.3f}'.format(mean_R2))
    print('best model parameter: ', cv.best_estimator_, '\n')

#Selecting the best model and evaluating it on our test dataset 
enet = ElasticNet(alpha=0.01)
enet.fit(X_train, y_train)
y_pred = enet.predict(X_test)

print('Test R2: {:.3f}'.format(r2_score(y_test, y_pred)))
print('Test RMSE: {:.3f}'.format((mean_squared_error(y_test, y_pred))**(1/2)))

#we can see from this that the best fit and the predictor are somewhat similar and are good predictions
from yellowbrick.regressor import prediction_error 

visualizer = prediction_error(enet, X_train, y_train, X_test, y_test)

#feature selection based on the coefficient(weight) values in the trained model
importance = np.abs(enet.coef_)

features = X.columns

feat_importance = pd.DataFrame([features, importance]).transpose()
feat_importance.rename(columns = {0: 'feature', 1: 'coefficient'}, inplace=True)
feat_importance.sort_values(by = 'coefficient', ascending = False)

"""# Thus the top three factors that are contributing to the preventable hospitalizations are -


*  Premature age-adjusted mortality
*  Poor or fair health
*  Diabetes prevalence

#Let's also try to find a relationship within these variables - 


*   These variables within themselves too are correlated therefore focussing on an overall picture in terms of eating habits and lifestyle can have an signigicant impact on preventable hospitalization 




"""

sns.pairplot(data[['Premature age-adjusted mortality raw value', 'Poor or fair health raw value', 'Diabetes prevalence raw value']])